{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "\n",
    "# MODEL_GPT = 'gpt-4o-mini'\n",
    "# MODEL_LLAMA = 'llama3.2'\n",
    "# print(\"üõ†Ô∏è  Script ba≈üladƒ±\")\n",
    "\n",
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è  Script ba≈üladƒ±\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "#for ollama client\n",
    "# client = ollama.Client()\n",
    "print(\"üõ†Ô∏è  Script ba≈üladƒ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53761722-486f-4be5-b489-85bddcbfa6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API anahtar uzunluƒüu: 164\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI API anahtar uzunluƒüu:\", len(openai.api_key or \"\"))\n",
    "if not openai.api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY ayarlƒ± deƒüil!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf770aaa-9c98-436a-be70-598320a9757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Script ba≈üladƒ±\n",
      "OpenAI API anahtar uzunluƒüu: 164\n",
      "\n",
      "--- GPT simple response (no stream) ---\n",
      "This line of code is using a generator function in Python with the `yield from` expression in conjunction with a set comprehension. Let‚Äôs break it down step-by-step:\n",
      "\n",
      "1. **Set Comprehension:** The part `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension. It iterates over an iterable called `books`, which is expected to be a collection (like a list) of dictionaries (where each dictionary represents a book). \n",
      "\n",
      "   - `book.get(\"author\")`: For each dictionary (book), this retrieves the value associated with the key `\"author\"`. Using `get()` instead of direct indexing (like `book[\"author\"]`) is helpful because it returns `None` (or a specified default value) if the key doesn‚Äôt exist, instead of raising a `KeyError`.\n",
      "   - `if book.get(\"author\")`: This condition filters out any authors that are `None` or falsey (i.e., empty strings, etc.), ensuring that only valid author names are included in the final set.\n",
      "\n",
      "   The result of the set comprehension is a set of unique author names from the `books` collection.\n",
      "\n",
      "2. **`yield from`:** The `yield from` expression is used inside a generator function to yield all values from the iterable that follows it. In this case, the iterable is the set produced by the set comprehension.\n",
      "\n",
      "3. **Purpose of the Code:**\n",
      "   - The entire expression will yield each unique author from the `books` collection one by one as they are requested. Since it's a set, it ensures that each author will only be yielded once, even if multiple books have the same author.\n",
      "   - Using `yield from` here makes it easy to iterate over the unique authors when this generator function is called.\n",
      "\n",
      "### Summary\n",
      "In summary, this line of code extracts unique author names from a collection of books and yields each name one at a time. This can be particularly useful in scenarios where you want to process or display unique authors from a list of books while conserving memory, as you only create the authors as needed instead of creating a complete list upfront.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- GPT (gpt-4o-mini) streaming response ---\n",
      "GPT √ßaƒürƒ±sƒ±nda hata: AttributeError(\"'ChoiceDelta' object has no attribute 'get'\")\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Ollama Llama (llama3.2) response ---\n",
      "model='llama3.2' created_at='2025-07-10T09:17:10.6621549Z' done=True done_reason='stop' total_duration=37153806800 load_duration=253702200 prompt_eval_count=52 prompt_eval_duration=544585000 eval_count=379 eval_duration=36352395100 message=Message(role='assistant', content='This line of code is written in Python and utilizes a feature called \"context managers\" or more specifically, \"generators with the `yield from` statement\".\\n\\nHere\\'s a breakdown:\\n\\n- `{book.get(\"author\") for book in books if book.get(\"author\")}`: This part is a generator expression. It generates an iterator that produces values based on some conditions.\\n\\n  - `\"book.get(\"author\")\"`: For each book, it tries to get the value of \"author\" from the book dictionary and yields this value.\\n  \\n  - `\"for book in books if book.get(\"author\")\"`: This filters out any books where \"author\" is not present. The condition `if book.get(\"author\")` ensures that only books with an author are included.\\n\\n- `yield from {book.get(\"author\") for book in books if book.get(\"author\")}`: This line uses the `yield from` statement, which is a feature introduced in Python 3.3. It takes an iterator and \"yield\" values from it one at a time, as if they were generated by the generator expression itself.\\n\\nIn summary, this code yields the authors of all books where the book has an author key present in its dictionary. This can be used for example to process each book individually without having to iterate over the entire collection and then filter out the ones with no author.\\n\\nHere\\'s a more concrete example:\\n\\n```python\\nbooks = [\\n    {\"title\": \"Book 1\", \"author\": \"Author 1\"},\\n    {\"title\": \"Book 2\", \"author\": None},\\n    {\"title\": \"Book 3\"}\\n]\\n\\nfor author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\\n    print(author)\\n```\\n\\nThis would output:\\n\\n```\\nAuthor 1\\nAuthor 1\\n```', images=None, tool_calls=None)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import openai\n",
    "import ollama\n",
    "\n",
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "# set up environment\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "# (Opsiyonel) Ollama client:\n",
    "# client = ollama.Client()\n",
    "\n",
    "# here is the question; type over this to ask something new\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "# --- Basit GPT testi (stream=False) ---\n",
    "def ask_gpt_simple(q):\n",
    "    print(\"\\n--- GPT simple response (no stream) ---\")\n",
    "    try:\n",
    "        resp = openai.chat.completions.create(\n",
    "            model=MODEL_GPT,\n",
    "            messages=[{\"role\": \"user\", \"content\": q}],\n",
    "            stream=False\n",
    "        )\n",
    "        print(resp.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(\"Basit GPT test hatasƒ±:\", repr(e))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# --- GPT streaming testi ---\n",
    "def ask_gpt_stream(q):\n",
    "    print(f\"\\n--- GPT ({MODEL_GPT}) streaming response ---\")\n",
    "    try:\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL_GPT,\n",
    "            messages=[{\"role\": \"user\", \"content\": q}],\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.get(\"content\")\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(\"GPT √ßaƒürƒ±sƒ±nda hata:\", repr(e))\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# --- Ollama Llama testi ---\n",
    "def ask_llama(q):\n",
    "    print(f\"\\n--- Ollama Llama ({MODEL_LLAMA}) response ---\")\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL_LLAMA,\n",
    "            messages=[{\"role\": \"user\", \"content\": q}]\n",
    "        )\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(\"Ollama √ßaƒürƒ±sƒ±nda hata:\", repr(e))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Debug ba≈ülangƒ±√ß √ßƒ±ktƒ±larƒ±\n",
    "    print(\"üõ†Ô∏è Script ba≈üladƒ±\")\n",
    "    print(\"OpenAI API anahtar uzunluƒüu:\", len(openai.api_key or \"\"))\n",
    "    if not openai.api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY ayarlƒ± deƒüil!\")\n",
    "\n",
    "    # 1) Stream kapalƒ± basit GPT testi\n",
    "    ask_gpt_simple(question)\n",
    "    # 2) Stream a√ßƒ±k GPT testi\n",
    "    ask_gpt_stream(question)\n",
    "    # 3) Ollama testi\n",
    "    ask_llama(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è  Script ba≈üladƒ±\n"
     ]
    }
   ],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üõ†Ô∏è  Script ba≈üladƒ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è  Script ba≈üladƒ±\n"
     ]
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "def ask_gpt_stream(q):\n",
    "    print(f\"\\n--- GPT ({MODEL_GPT}) streaming response ---\")\n",
    "    stream = openai.ChatCompletion.create(\n",
    "        model = MODEL_GPT,\n",
    "        messages = [{\"role\":\"user\",\"content\": q}],\n",
    "        stream = True\n",
    "    )\n",
    "    # Gelen par√ßalarƒ± basit√ße ekrana yazdƒ±rƒ±yoruz\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta\n",
    "        if delta.get(\"content\"):\n",
    "            print(delta.content, end=\"\", flush=True)\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "print(\"üõ†Ô∏è  Script ba≈üladƒ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPT (gpt-4o-mini) streaming response ---\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIRemovedInV1\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# GPT ile streaming yanƒ±t\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mask_gpt_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Llama ile normal yanƒ±t\u001b[39;00m\n\u001b[32m     17\u001b[39m     ask_llama(question)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mask_gpt_stream\u001b[39m\u001b[34m(q)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask_gpt_stream\u001b[39m(q):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- GPT (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_GPT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) streaming response ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     stream = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_GPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Gelen par√ßalarƒ± basit√ße ekrana yazdƒ±rƒ±yoruz\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[39m, in \u001b[36mAPIRemovedInV1Proxy.__call__\u001b[39m\u001b[34m(self, *_args, **_kwargs)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *_args: Any, **_kwargs: Any) -> Any:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol=\u001b[38;5;28mself\u001b[39m._symbol)\n",
      "\u001b[31mAPIRemovedInV1\u001b[39m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "def ask_llama(q):\n",
    "    print(f\"\\n--- Ollama Llama ({MODEL_LLAMA}) response ---\")\n",
    "    # Senkron √ßaƒürƒ±\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[{\"role\": \"user\", \"content\": q}]\n",
    "    )\n",
    "    print(response)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # GPT ile streaming yanƒ±t\n",
    "    ask_gpt_stream(question)\n",
    "    # Llama ile normal yanƒ±t\n",
    "    ask_llama(question)\n",
    "print(\"üõ†Ô∏è  Script ba≈üladƒ±\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6a4fc-53fc-4830-8b0c-c0e5a5ca1408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
